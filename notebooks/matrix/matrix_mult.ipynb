{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXNz4F8aMnQo",
        "outputId": "936b89d6-92c7-4192-aa6e-c28d9576112d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  2 02:25:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws5X62mSMa2U",
        "outputId": "cb046717-c9f4-4617-8eea-94f0a53de32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/422.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja --quiet\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drWiSzpMMa2V",
        "outputId": "58895547-9376-472d-e230-a61b7dc9bf7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file ./cuda_build/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module matrix_dot_product_v1...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module matrix_dot_product_v1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "torch.Size([512, 512])\n",
            "tensor([[22.6350, 24.4277, 23.8745,  ..., 26.1167, 23.2366, 22.5866],\n",
            "        [24.0192, 26.2986, 26.0992,  ..., 26.8558, 24.6046, 24.6146],\n",
            "        [22.2251, 27.7167, 25.3181,  ..., 27.1882, 23.4455, 23.3273],\n",
            "        ...,\n",
            "        [23.9101, 26.3441, 25.6326,  ..., 26.2920, 24.3043, 24.0408],\n",
            "        [24.4643, 25.8847, 25.4565,  ..., 26.5364, 23.0210, 23.7693],\n",
            "        [23.7305, 25.9510, 23.9274,  ..., 26.0556, 23.0160, 21.8001]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "build_directory = './cuda_build'\n",
        "if os.path.exists(build_directory):\n",
        "    shutil.rmtree(build_directory)\n",
        "if not os.path.exists(build_directory):\n",
        "    os.makedirs(build_directory)\n",
        "\n",
        "def compile_extension():\n",
        "    cuda_source = Path(\"/content/drive/MyDrive/Cuda_Learning/kernels/matrix_mult.cu\").read_text()\n",
        "    cpp_source = \"torch::Tensor matrix_mult(torch::Tensor matrix_a, torch::Tensor matrix_b);\"\n",
        "\n",
        "    # Load the CUDA kernel as a PyTorch extension\n",
        "    dot_product_extension = load_inline(\n",
        "        name=\"matrix_dot_product_v1\",\n",
        "        cpp_sources=cpp_source,\n",
        "        cuda_sources=cuda_source,\n",
        "        functions=[\"matrix_mult\"],\n",
        "        with_cuda=True,\n",
        "        extra_cuda_cflags=[\"-O2\"],\n",
        "        verbose=True,\n",
        "        build_directory=build_directory,\n",
        "    )\n",
        "    return dot_product_extension\n",
        "\n",
        "kernel = compile_extension()\n",
        "\n",
        "def _main():\n",
        "    matrix_a = torch.rand(512, 100, device='cuda')\n",
        "    matrix_b = torch.rand(100, 512, device='cuda')\n",
        "\n",
        "    # Perform matrix multiplication using the CUDA kernel\n",
        "    result_cuda = kernel.matrix_mult(matrix_a, matrix_b)\n",
        "    # Perform matrix multiplication using PyTorch for comparison\n",
        "    result_pytorch = torch.matmul(matrix_a, matrix_b)\n",
        "\n",
        "    print(torch.allclose(result_cuda, result_pytorch))\n",
        "    print(result_cuda.shape)\n",
        "    print(result_cuda)\n",
        "\n",
        "\n",
        "_main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.lib.xla_bridge as xb\n",
        "print(xb.get_backend().platform)\n",
        "import os\n",
        "os.environ['JAX_PLATFORM_NAME'] = 'gpu'"
      ],
      "metadata": {
        "id": "kldi3KKHIHOO",
        "outputId": "52e6c654-64d9-44e8-919c-4e834e927d61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-681d50e155f6>:2: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
            "  print(xb.get_backend().platform)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import time\n",
        "\n",
        "def benchmark_matrix_mult(size):\n",
        "    \"\"\"Benchmarks matrix multiplication for different methods.\"\"\"\n",
        "\n",
        "    # Create input matrices\n",
        "    # By default, np.random.rand creates arrays with the float64 data type (double-precision floating-point numbers).\n",
        "    matrix_a_np = np.random.rand(size, size).astype(np.float32)\n",
        "    matrix_b_np = np.random.rand(size, size).astype(np.float32)\n",
        "\n",
        "    matrix_a_torch_cpu = torch.from_numpy(matrix_a_np).cpu()\n",
        "    matrix_b_torch_cpu = torch.from_numpy(matrix_b_np).cpu()\n",
        "\n",
        "    matrix_a_torch_cuda = torch.from_numpy(matrix_a_np).cuda()\n",
        "    matrix_b_torch_cuda = torch.from_numpy(matrix_b_np).cuda()\n",
        "\n",
        "    matrix_a_jax = jnp.array(matrix_a_np)\n",
        "    matrix_b_jax = jnp.array(matrix_b_np)\n",
        "\n",
        "    # Custom CUDA kernel\n",
        "    start_time = time.time()\n",
        "    result_cuda = kernel.matrix_mult(matrix_a_torch_cuda, matrix_b_torch_cuda)\n",
        "    cuda_time = time.time() - start_time\n",
        "\n",
        "    # NumPy\n",
        "    start_time = time.time()\n",
        "    result_numpy = np.matmul(matrix_a_np, matrix_b_np)\n",
        "    numpy_time = time.time() - start_time\n",
        "\n",
        "    # PyTorch CPU\n",
        "    start_time = time.time()\n",
        "    result_pytorch_cpu = torch.matmul(matrix_a_torch_cpu, matrix_b_torch_cpu)\n",
        "    pytorch_cpu_time = time.time() - start_time\n",
        "\n",
        "    # PyTorch CUDA\n",
        "    start_time = time.time()\n",
        "    result_pytorch_cuda = torch.matmul(matrix_a_torch_cuda, matrix_b_torch_cuda)\n",
        "    pytorch_cuda_time = time.time() - start_time\n",
        "\n",
        "\n",
        "    # JAX\n",
        "    # JAX uses JIT compilation to optimize computations.\n",
        "    #  The first time a function is executed, JAX traces it and compiles an optimized version. Subsequent executions of the same function will be faster.\n",
        "    start_time = time.time()\n",
        "    result_jax = jnp.matmul(matrix_a_jax, matrix_b_jax)\n",
        "    jax_time = time.time() - start_time\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Matrix size: {size}x{size}\")\n",
        "    print(f\"Custom CUDA kernel: {cuda_time:.4f} seconds\")\n",
        "    print(f\"NumPy: {numpy_time:.4f} seconds\")\n",
        "    print(f\"PyTorch CPU: {pytorch_cpu_time:.4f} seconds\")\n",
        "    print(f\"PyTorch CUDA: {pytorch_cuda_time:.4f} seconds\")\n",
        "    print(f\"JAX: {jax_time:.4f} seconds\")\n",
        "\n",
        "benchmark_matrix_mult(5555)"
      ],
      "metadata": {
        "id": "SRczxsTSFFr5",
        "outputId": "4a38d72b-7070-40f7-8a3d-733e61e7d0f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix size: 5555x5555\n",
            "Custom CUDA kernel: 0.0001 seconds\n",
            "NumPy: 5.8842 seconds\n",
            "PyTorch CPU: 2.8816 seconds\n",
            "PyTorch CUDA: 0.0002 seconds\n",
            "JAX: 0.0004 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}